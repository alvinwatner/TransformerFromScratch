{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "||Challenge||Transformer_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqVb_LT3zK-A"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "'''Embedder class handle the word embedding followed by the positional encoding'''\n",
        "class Embedder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.embedding_layer  = nn.Embedding(vocab_size, embedding_size) \n",
        "    PE                    = torch.zeros(max_seq_len, embedding_size) \n",
        "\n",
        "    for pos in range(max_seq_len):\n",
        "      for i in range(0, embedding_size, 2):\n",
        "        PE[pos, i]     = math.sin(pos/10000**((2 * i)/embedding_size))       #if i even\n",
        "        PE[pos, i + 1] = math.cos(pos/10000**((2 * (i + 1))/embedding_size)) #if i odd\n",
        "\n",
        "    #PE.shape = (1, max_seq_len, embedding_size)\n",
        "    PE = PE.unsqueeze(0)\n",
        "    self.register_buffer('PE', PE) #save PE in 'state_dict', but not trained by optimizer\n",
        "  \n",
        "  def forward(self, inp):\n",
        "    #inp.shape = (batch_size, seq_len)\n",
        "    seq_len  = inp.shape[1]\n",
        "\n",
        "    #embedded.shape = (batch_size, seq_len, embedding_size)\n",
        "    embedded            = self.embedding_layer(inp)\n",
        "    positional_embedded = self.PE[:, :seq_len] + embedded #Positional Encoding\n",
        "\n",
        "    return positional_embedded\n",
        "\n",
        "class Self_Attention(nn.Module):\n",
        "  def __init__(self, embedding_size = 256, max_seq_len = 100):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "    self.mask_matrix = torch.full((max_seq_len, max_seq_len), float(\"-1e20\"))\n",
        "    self.mask_matrix = torch.triu(self.mask_matrix, diagonal = 1)\n",
        "    \n",
        "    self.Linear      = nn.Linear(embedding_size, embedding_size)\n",
        "    self.Softmax     = nn.Softmax(dim = 2) #dim 2 refer to embedding_size -> (batch_size, seq_len, embedding_size)\n",
        "      \n",
        "  def forward(self, query, key, value, masked = False):\n",
        "    #query, key, value shape = (batch_size, seq_len, embedding_size)\n",
        "    query = self.Linear(query)\n",
        "    key   = self.Linear(key)\n",
        "    value = self.Linear(value)\n",
        "\n",
        "    #key_Transpose = (batch_size, embedding_size, seq_len)\n",
        "\n",
        "    key_Transpose = torch.transpose(key, 1, 2)\n",
        "    score         = torch.bmm(query, key_Transpose)\n",
        "    #scaled_score = (batch_size, seq_len, seq_len)\n",
        "    scaled_score  = score / math.sqrt(self.embedding_size)\n",
        "\n",
        "    if masked:\n",
        "      scaled_score = scaled_score + self.mask_matrix #this is the only difference, ho yea\n",
        "      attn_weight  = self.Softmax(scaled_score)\n",
        "    else:\n",
        "      attn_weight   = self.Softmax(scaled_score) \n",
        "    \n",
        "    output        = torch.bmm(attn_weight, value)\n",
        "\n",
        "    return output\n",
        "\n",
        "''''''\n",
        "class MultiHeaded_Attention(nn.Module):\n",
        "  def __init__(self, embedding_size, max_seq_len, num_heads = 8):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim  = embedding_size // self.num_heads\n",
        "\n",
        "    assert (self.num_heads * self.head_dim == embedding_size), \"the division between 'embedding_size' with 'num_heads' must be integer\"\n",
        "\n",
        "    self.self_attn        = Self_Attention(embedding_size = self.head_dim, max_seq_len = max_seq_len)\n",
        "    self.Linear           = nn.Linear(embedding_size, embedding_size)\n",
        "  \n",
        "  def forward(self, query = None, key = None, value = None, masked = False):\n",
        "    #embedded.shape = (batch_size, seq_len, embedding_dim) \n",
        "    batch_size      = query.shape[0]\n",
        "    seq_len         = query.shape[1]\n",
        "    \n",
        "    #Split Query, Key, and Value into N number of head.\n",
        "    splited_query = query.reshape(self.num_heads, batch_size, seq_len, self.head_dim)\n",
        "    splited_key   = key.reshape(self.num_heads, batch_size, seq_len, self.head_dim)\n",
        "    splited_value = value.reshape(self.num_heads, batch_size, seq_len, self.head_dim)\n",
        "\n",
        "    #initialize empty tensor to store final concatentation\n",
        "    joined_heads = torch.Tensor\n",
        "\n",
        "    #After get splited, feed each head into self_attn layer and concatenate the output.\n",
        "    for h in range(self.num_heads):\n",
        "      query = splited_query[h]\n",
        "      key   = splited_key[h]\n",
        "      value = splited_value[h]\n",
        "\n",
        "      #head_out.shape = (batch_size, seq_len, head_dim)\n",
        "      if masked:\n",
        "        head_out        = self.self_attn(query, key, value, masked = True)\n",
        "      else:\n",
        "        head_out        = self.self_attn(query, key, value)\n",
        "\n",
        "      if h == 0:\n",
        "        joined_heads = head_out\n",
        "      else:\n",
        "        #Concatenate the current head_out with previous joined_heads\n",
        "        joined_heads = torch.cat((head_out, joined_heads),  dim = 2)    \n",
        "    \n",
        "    joined_heads = self.Linear(joined_heads)\n",
        "\n",
        "    return joined_heads\n",
        "\n",
        "\n",
        "''''''\n",
        "class Encoder_Block(nn.Module):\n",
        "  def __init__(self, embedding_size, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.MultiHeaded_Attention = MultiHeaded_Attention(embedding_size, \n",
        "                                                       max_seq_len)\n",
        "    \n",
        "    self.norm                  = nn.LayerNorm(embedding_size)\n",
        "    \n",
        "    self.feed_forward          = nn.Sequential(\n",
        "                                 nn.Linear(embedding_size, embedding_size),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(embedding_size, embedding_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, encoder_in):\n",
        "    out = self.MultiHeaded_Attention(query = encoder_in,\n",
        "                                     key   = encoder_in,\n",
        "                                     value = encoder_in\n",
        "                                     )\n",
        "    \n",
        "    norm_out1 = self.norm(out + encoder_in)\n",
        "    FF_out    = self.feed_forward(norm_out1)\n",
        "    norm_out2 = self.norm(FF_out + norm_out1)\n",
        "\n",
        "    return norm_out2                               \n",
        "\n",
        "''''''\n",
        "class Decoder_Block(nn.Module):\n",
        "  def __init__(self, embedding_size, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.MultiHeaded_Attention = MultiHeaded_Attention(embedding_size,\n",
        "                                                       max_seq_len)\n",
        "    \n",
        "    self.norm                  = nn.LayerNorm(embedding_size)\n",
        "    \n",
        "    self.feed_forward          = nn.Sequential(\n",
        "                                 nn.Linear(embedding_size, embedding_size),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(embedding_size, embedding_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, encoder_out, decoder_in):\n",
        "    out = self.MultiHeaded_Attention(query  = decoder_in,\n",
        "                                     key    = decoder_in,\n",
        "                                     value  = decoder_in,\n",
        "                                     masked = True)\n",
        "    \n",
        "    norm_out1 = self.norm(out + decoder_in)\n",
        "\n",
        "    out = self.MultiHeaded_Attention(query = encoder_out,\n",
        "                                     key   = encoder_out,\n",
        "                                     value = norm_out1)\n",
        "    \n",
        "    \n",
        "    norm_out2 = self.norm(out + norm_out1)\n",
        "\n",
        "    FF_out     = self.feed_forward(norm_out2)\n",
        "    norm_out3  = self.norm(FF_out + norm_out2)\n",
        "\n",
        "    return norm_out3\n",
        "\n",
        "\n",
        "''''''\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, SRCvocab_size, TRGvocab_size, embedding_size, max_seq_len, num_layers = 6):\n",
        "    super().__init__()\n",
        "    self.num_layers    = num_layers\n",
        "    self.max_seq_len   = max_seq_len\n",
        "\n",
        "    self.SRC_embedder  = Embedder(SRCvocab_size, embedding_size, max_seq_len)\n",
        "    self.TRG_embedder  = Embedder(TRGvocab_size, embedding_size, max_seq_len)\n",
        "    \n",
        "    self.encoder       = Encoder_Block(embedding_size, max_seq_len)\n",
        "    self.decoder       = Decoder_Block(embedding_size, max_seq_len)\n",
        "\n",
        "    self.linear        = nn.Linear(embedding_size, TRGvocab_size)\n",
        "    self.softmax       = nn.Softmax(dim = 2)\n",
        "\n",
        "  def forward(self, src, trg):  \n",
        "    batch_size = src.shape[0]\n",
        "\n",
        "    Fixed_src  = torch.ones((batch_size, self.max_seq_len), dtype = torch.long)\n",
        "    SRCseq_len = src.shape[1]\n",
        "    Fixed_src[:, :SRCseq_len] = src\n",
        "\n",
        "    Fixed_trg  = torch.ones((batch_size, self.max_seq_len), dtype = torch.long)\n",
        "    TRGseq_len = trg.shape[1]\n",
        "    Fixed_trg[:, :TRGseq_len] = trg\n",
        "    \n",
        "    SRCembedded = self.SRC_embedder(Fixed_src)\n",
        "    TRGembedded = self.TRG_embedder(Fixed_trg)\n",
        "    encoder_out = self.encoder(SRCembedded)\n",
        "    decoder_out = self.decoder(encoder_out, TRGembedded)\n",
        "\n",
        "    for layer in range(self.num_layers - 1):\n",
        "      encoder_out = self.encoder(encoder_out)\n",
        "      decoder_out = self.decoder(encoder_out, decoder_out)\n",
        "\n",
        "    decoder_out = self.linear(decoder_out)\n",
        "    decoder_out = self.softmax(decoder_out)\n",
        "\n",
        "  \n",
        "    return decoder_out"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIgtvZLCjTAJ"
      },
      "source": [
        "embedding_size = 256\n",
        "SRCvocab_size  = 900\n",
        "TRGvocab_size  = 800\n",
        "max_seq_len    = 100\n",
        "\n",
        "transformer = Transformer(SRCvocab_size, TRGvocab_size, embedding_size, max_seq_len)\n",
        "\n",
        "src = torch.LongTensor(([1, 2, 3], [1, 2, 3]))\n",
        "trg = torch.LongTensor(([1, 2, 3], [1, 2, 3]))\n",
        "out = transformer(src, trg)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW0F5GYcnauY",
        "outputId": "424777cc-09c7-4eb9-c665-246b95a73aae"
      },
      "source": [
        "print(sum(out[0,0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0000, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}